{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa-gEAOwRhKs"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas numpy transformers torch scikit-learn tqdm openpyxl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CompanyMentionDataset(Dataset):\n",
        "  def __init__(self, texts, companies, labels, tokenizer, max_length=512):\n",
        "    self.texts = texts\n",
        "    self.companies = companies\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self): return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx]\n",
        "    company = self.companies[idx]\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    modified_text = text.replace(company, f\"<company>{company}</company>\")\n",
        "\n",
        "    encoding = self.tokenizer(\n",
        "      modified_text,\n",
        "      return_tensors='pt',\n",
        "      max_length=self.max_length,\n",
        "      padding='max_length',\n",
        "      truncation=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'labels': torch.tensor(label)\n",
        "    }"
      ],
      "metadata": {
        "id": "2Hn8mizkSbeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "  sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "  df['label'] = df['tonality'].map(sentiment_map)\n",
        "\n",
        "  def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\-\\\"\\«\\»]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "  df['clean_sentence'] = df['sentence'].apply(clean_text)\n",
        "  return df\n",
        "\n",
        "def train_model(train_dataloader, model, optimizer, device, num_epochs=10):\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}')\n",
        "\n",
        "    for batch in progress_bar:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        labels=labels\n",
        "      )\n",
        "      loss = outputs.loss\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1)})\n",
        "\n",
        "def evaluate_model(eval_dataloader, model, device):\n",
        "  model.eval()\n",
        "  all_preds, all_labels = [], []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in eval_dataloader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      preds = torch.argmax(outputs.logits, dim=1)\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "  return all_preds, all_labels"
      ],
      "metadata": {
        "id": "_kCT1oL2UK68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  df = preprocess_data(pd.read_excel('/content/sample_data/ru_data_test.xlsx')) # измените путь до вашего .xlsx файла\n",
        "\n",
        "  train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "  model_name = \"DeepPavlov/rubert-base-cased-sentence\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "  special_tokens = {'additional_special_tokens': ['<company>', '</company>']}\n",
        "  tokenizer.add_special_tokens(special_tokens)\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "  train_dataset = CompanyMentionDataset(\n",
        "    train_df['clean_sentence'].tolist(),\n",
        "    train_df['object'].tolist(),\n",
        "    train_df['label'].tolist(),\n",
        "    tokenizer\n",
        "  )\n",
        "  test_dataset = CompanyMentionDataset(\n",
        "    test_df['clean_sentence'].tolist(),\n",
        "    test_df['object'].tolist(),\n",
        "    test_df['label'].tolist(),\n",
        "    tokenizer\n",
        "  )\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "  train_model(train_dataloader, model, optimizer, device)\n",
        "  preds, labels = evaluate_model(test_dataloader, model, device)\n",
        "  label_names = ['negative', 'neutral', 'positive']\n",
        "  print(classification_report(labels, preds, target_names=label_names))\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "def predict_sentiment(text, company, model, tokenizer, device):\n",
        "  model.eval()\n",
        "  modified_text = text.replace(company, f\"<company>{company}</company>\")\n",
        "\n",
        "  encoding = tokenizer(\n",
        "    modified_text,\n",
        "    return_tensors='pt',\n",
        "    max_length=512,\n",
        "    padding='max_length',\n",
        "    truncation=True\n",
        "  )\n",
        "  with torch.no_grad():\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    prediction = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "  sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "  return sentiment_map[prediction.item()]"
      ],
      "metadata": {
        "id": "wQqTsQmFXxF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  model, tokenizer = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y39K_ueddEGL",
        "outputId": "1df3b825-ba11-4ccc-ba3e-533839bb9dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 11/11 [00:31<00:00,  2.87s/it, loss=1.03]\n",
            "Epoch 2: 100%|██████████| 11/11 [00:30<00:00,  2.75s/it, loss=0.824]\n",
            "Epoch 3: 100%|██████████| 11/11 [00:31<00:00,  2.89s/it, loss=0.531]\n",
            "Epoch 4: 100%|██████████| 11/11 [00:33<00:00,  3.02s/it, loss=0.328]\n",
            "Epoch 5: 100%|██████████| 11/11 [00:33<00:00,  3.03s/it, loss=0.162]\n",
            "Epoch 6: 100%|██████████| 11/11 [00:33<00:00,  3.02s/it, loss=0.0999]\n",
            "Epoch 7: 100%|██████████| 11/11 [00:32<00:00,  2.92s/it, loss=0.0553]\n",
            "Epoch 8: 100%|██████████| 11/11 [00:34<00:00,  3.16s/it, loss=0.0348]\n",
            "Epoch 9: 100%|██████████| 11/11 [00:33<00:00,  3.06s/it, loss=0.0251]\n",
            "Epoch 10: 100%|██████████| 11/11 [00:33<00:00,  3.03s/it, loss=0.0195]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.55      0.67        11\n",
            "     neutral       0.00      0.00      0.00         3\n",
            "    positive       0.47      0.78      0.58         9\n",
            "\n",
            "    accuracy                           0.57        23\n",
            "   macro avg       0.44      0.44      0.42        23\n",
            "weighted avg       0.59      0.57      0.55        23\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Описание подхода:\n",
        "Подход состоит в файн-тюнинге предобученной модели RuBERT.\n",
        "Также были добавлены:\n",
        "1. Маркеры < company> и </ company> вокруг предложений для лучшей обработки текстов об определенной компании\n",
        "2. Очистка данных, которая заключается в удалении специальных символов и сохранение знаков препинания\n",
        "\n",
        "## Проблемы:\n",
        "1. Маленький датасет, из-за которого получилось очень неточное распределение классов"
      ],
      "metadata": {
        "id": "BLgf8MYygf_G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l1FFP_VCiL9d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}